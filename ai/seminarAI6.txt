AI seminar 6
-------------

Normalize data

1) Stohastical
s - sample data
m - the average S
std - standard deviation s
V = (v-m)/std  
v belongs to S

2)min - max

m - min S
M - max S
v belongs to S -> v belongs to [m,M] -> V belongs to [VMIN, VMAX]
VMIN = -1, VMAX = 1
V = VMIN + (v - m)/(M-m) * (VMAX - VMIN)

#SWEET CODE AHEAD

class Neuron:
	def __init__(self,n):
		self.nInputs = n
		self.w = [random()*2-1 for i in range (def nInputs)]
		self.o = 0
		self.err = 0
	
	def activare(self, u):
		s = sum([u[i]*self.w[i] for i in range(self.nInputs)])
		self.o = s #linear
		
		#self.o = 1/(1+exp(-s))  #the sigmoid thing 
		
	def setErr(self, v):
		#linear
		self.err = v 
		
		#sigmoid
		#self.err = self.o*(1-self.o)*v)
		
		
class Layer:
	def __init__(self, m, v):
		self.nNuron = m
		self.neurons = [Neuron(n) for k in range(self.nNeurons)]
		

class Network:
	def __init__(self, a, b, c, d):
		self.nInputs = a
		self.nOutputs = b
		self.nNPHL = c
		self.nHL = d
		self.layers = [Layer(self.nInputs, 0)
		self.layers += [Layer(self.nNPHL, self.nInputs)]
		self.layers += [Layer(self.nNPHL, self.nNPHL) for i in range(self.nHL - 1)]
		self.layers += [Layer(self.nOutputs, self.nNPHL)]
	
	#u - intrarile (input)
	def activate(self,u):
		i = 0
		for neuron in self.layers[0].neurons:
			neuron.o = u[i]
		
		for l in range (1, self.nHL + 2):
			info = [n2.o for n2 in self.layers[l - 1].neurons] 
			for n in self.layers[l].neurons:
				m.activate(info)
				
	def errorBackPropagate(self, error):	
		for l in range(self.nHL + 1, 0, -1):
			i = 0
			for n in self.layers[l].neurons:
				if l == self.nHL + 1:
					n.setErr(error[i]
				else:
					s = sum([n2.w[i]*n2.err) for n2 in self.layers[l+1].neurons)
					n.setErr(s)	
			i = i + 1
			for j in range(n.Inputs):
				n.w[j] = n.w[j] * learningRate * n.err * self.layers[l - 1].neurons[j].o 
						